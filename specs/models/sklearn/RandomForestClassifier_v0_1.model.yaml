name: sklearnRandomForestClassifierBroadNucleusData
description: Random Forest Classifier form scikit-learn
cite:
  - text: L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001
    url: https://scikit-learn.org/stable/modules/generated/sklearnbased.ensemble.RandomForestClassifier.html
authors:
  - Fynn Beuttenmueller
documentation: sklearnbased.md
tags: [test, rf, random, forest, example]
license: MIT

format_version: 0.1.0
language: python
framework: scikit-learn

source: pybio.sklearn.models.sklearnbased.RandomForestClassifier
optional_kwargs:
  channel_indices: [null]
  n_estimators: 10
  criterion: gini
  max_depth: null
  min_samples_split: 2
  min_samples_leaf: 1
  min_weight_fraction_leaf: 0.0
  max_features: auto
  max_leaf_nodes: null
  min_impurity_decrease: 0.0
  bootstrap: true
  oob_score: false
  n_jobs: 1
  random_state: 0
  verbose: 0
  warm_start: false
  class_weight: null

test_input: test_input_raw.npy
test_output: test_output.npy
covers: [] # ./nuclei_thumbnail.png

inputs:
  - name: raw
    axes: byx
    data_type: float32
    data_range: [-inf, inf]
    shape:
      min: [1, 96, 96]
      step: [0, 1, 1]

outputs:
  - name: probabilities
    axes: byx
    data_type: float32
    data_range: [0, 1]
    halo: [0, 32, 32]
    shape:
      reference_input: raw
      scale: [1, 1, 1]
      offset: [0, 0, 0]

prediction:
    preprocess: []
    weights:
        source: rf_v0.pickle
        hash: {sha256: abcdefghabcdefghabcdefghabcdefghabcdefghabcdefghabcdefghabcdefgh}
    postprocess: []
    dependencies: conda:../env_numpy.yaml


training:
  setup:
    samplers:
      - spec: ../../samplers/SequentialSamplerAlongDimension.sampler.yaml
        kwargs: {sample_dimensions: [0, 0]}
        readers:
          - spec: ../../readers/BroadNucleusDataBinarized.reader.yaml
            transformations:
              - spec: ../../transformations/NormalizeZeroMeanUnitVariance.transformation.yaml

  source: pybio.core.training.classic_fit.classic_fit
  required_kwargs: [pybio_model]
  optional_kwargs: {start: 0, batch_size: 1}
  # enable different ways of specifying the dependencies.
  # this would hold all training dependencies, e.g. as a frozen conda environment
  # or as a pom.xml
  dependencies: conda:./test_env.yaml  # this is a file to the dependencies
  description: todo describe training

config:
  future:
    0.3.0:
      weights_format: "pickle"
      weights:
        id: default
        name: toy examle weights
        description: "Trained as a toy example on a tiny subset of the broad nucleus data. Don't use for any real problem! (Use ilastik pixelclassification instead! ;-))"
        tags: [toy example]
      timestamp: "2020-10-23T16:00:42+00:00"
      git_repo: "https://github.com/bioimage-io/python-bioimage-io/tree/master/specs/models/sklearnbased"
